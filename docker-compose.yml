# docker‚Äëcompose.yml  ‚Äì  drop straight into your project root
# run with:  docker compose up -d --build

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - '11434:11434' # expose API to host
    volumes:
      - ollama-data:/root/.ollama # keep model cache

    # ‚îÄ‚îÄ bootstrap: start daemon ‚Üí wait ‚Üí pull model ‚îÄ‚îÄ
    entrypoint: |
      sh -e -c '
        # 1) start Ollama daemon in background
        ollama serve & pid=$!

        # 2) wait until the API responds
        until ollama list >/dev/null 2>&1; do
          echo "‚è≥  waiting for Ollama daemon‚Ä¶"
          sleep 1
        done

        # 3) pull the model if it isn‚Äôt cached
        echo "‚è¨  pulling deepseek‚Äër1:1.5b (first run only)‚Ä¶"
        ollama pull deepseek-r1:1.5b

        echo "üöÄ  Ollama ready"
        # 4) keep the daemon in foreground
        wait $pid
      '

    # health = API answers; web waits on this
    healthcheck:
      test: ['CMD', 'ollama', 'list']
      interval: 30s
      timeout: 10s
      retries: 10

  web:
    build: .
    container_name: nextjs
    restart: unless-stopped
    ports:
      - '3000:3000'
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    # optional: for Puppeteer stability
    shm_size: '1gb'

volumes:
  ollama-data:
